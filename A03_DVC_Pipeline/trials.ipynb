{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY_GPT4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-vision-preview\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"\"\"This image consists of a machine learning pipeline to be done using DVC and git. Look at the image and write me the stages in pointwise manner about what is expected to be done.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://i.postimg.cc/ydgWgHrG/Screenshot-2024-03-23-140231.png\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Based on the image you've provided, here are the stages of the machine learning pipeline presented in a pointwise manner:\\n\\n1. **Parameters Configuration**: Set the parameters such as year and number of locations (`n_locs`) in the `params.yaml` file.\\n\\n2. **Fetch Dataset**: Use a script named `download.py` to fetch the dataset from the NCEI portal using tools like `curl` or `wget`. The output of this stage is the downloaded CSV data files.\\n\\n3. **Prepare Ground Truth Data (GT)**: Utilizing a script `prepare.py`, extract monthly aggregates from the CSV data files to prepare the ground truth (GT) data. The inputs are the downloaded CSV data files, and the output is a monthly aggregated GT dataset.\\n\\n4. **Compute Monthly Aggregates**: Compute further monthly aggregates using the `process.py` script. The input required for this stage is a list of fields, and the output is a set of computed monthly aggregates.\\n\\n5. **Compare and Evaluate**: Finally, compare the aggregates and report the metric using the `evaluate.py` script. The inputs for this script are the computed monthly aggregates, and the output is a metric, specifically `R^2`.\\n\\nThis pipeline represents a typical data processing flow in a machine learning context, with the initial parameters setting the stage. Scripts are employed in each step to perform specific tasks such as data retrieval, preparation, processing, and evaluation. The overall process seems to be designed for reproducibility and version control, taking advantage of tools such as DVC (Data Version Control) and Git.\", role='assistant', function_call=None, tool_calls=None))\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the image you've provided, here are the stages of the machine learning pipeline presented in a pointwise manner:\n",
      "\n",
      "1. **Parameters Configuration**: Set the parameters such as year and number of locations (`n_locs`) in the `params.yaml` file.\n",
      "\n",
      "2. **Fetch Dataset**: Use a script named `download.py` to fetch the dataset from the NCEI portal using tools like `curl` or `wget`. The output of this stage is the downloaded CSV data files.\n",
      "\n",
      "3. **Prepare Ground Truth Data (GT)**: Utilizing a script `prepare.py`, extract monthly aggregates from the CSV data files to prepare the ground truth (GT) data. The inputs are the downloaded CSV data files, and the output is a monthly aggregated GT dataset.\n",
      "\n",
      "4. **Compute Monthly Aggregates**: Compute further monthly aggregates using the `process.py` script. The input required for this stage is a list of fields, and the output is a set of computed monthly aggregates.\n",
      "\n",
      "5. **Compare and Evaluate**: Finally, compare the aggregates and report the metric using the `evaluate.py` script. The inputs for this script are the computed monthly aggregates, and the output is a metric, specifically `R^2`.\n",
      "\n",
      "This pipeline represents a typical data processing flow in a machine learning context, with the initial parameters setting the stage. Scripts are employed in each step to perform specific tasks such as data retrieval, preparation, processing, and evaluation. The overall process seems to be designed for reproducibility and version control, taking advantage of tools such as DVC (Data Version Control) and Git.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
